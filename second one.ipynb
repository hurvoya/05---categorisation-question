{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir vu les différents types de nettoyage du texte possible dans le notebook précédent,\n",
    "</br>\n",
    "</br>(1) Nous allons maintenant étudier comment extraire l'information du texte pour le traitement ultérieur par des modèles de machine learning\n",
    "</br>\n",
    "</br>(2) Puis, nous modéliserons nos sujet avec une méthode non supervisé : Lattent Dirichlet Allocation (LDA).\n",
    "</br>\n",
    "</br>(3) Enfin, nous utiliserons des méthodes supervisées afin d'assigner à chaque document les tags associés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/miljan/predicting-tags-for-stackoverflow/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Green>Partie 1 : Création de features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>1. Préléminaires</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = grey>1.1 Contexte</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette Partie est dédiée à la Transformation de notre Texte et donc à la création de features exploitable pour le Machine Learning.\n",
    "</br>Nous allons Testser différentes méthodes de représentation de nos document :\n",
    "</br>\n",
    "</br>&emsp;<b>(1) Méthodes de comptage direct</b> (fréquence ou tf-idf) :\n",
    "</br>&emsp;&ensp;- Approche Bag of Words\n",
    "</br>&emsp;&ensp;- Approche n-gram (Bag of words généralisé)\n",
    "</br>&emsp;&ensp;- Approche TF-IDF\n",
    "</br>\n",
    "</br>&emsp;<b>(2) Méthode plongement de mot :</b>\n",
    "</br>&emsp;&ensp;- Word2Vec\n",
    "</br>&emsp;&ensp;- Doc2Vec\n",
    "</br>\n",
    "</br>&emsp;<b>(3) BERT</b>\n",
    "</br>\n",
    "</br>&emsp;<b>(4) USE</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = grey>1.2 Librairies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classiques\n",
    "import os, sys, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer #pour le bag of word et n-grams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #pour le tf idf\n",
    "from sklearn.manifold import TSNE #pour le tsne\n",
    "from sklearn.decomposition import LatentDirichletAllocation #pour la lda\n",
    "\n",
    "#Gensim & setup login\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) #set up login\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = grey>1.3 Chargement des données</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>title_body</th>\n",
       "      <th>clean_title_body</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>clean_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MyISAM versus InnoDB</td>\n",
       "      <td>I'm working on a projects which involves a lot...</td>\n",
       "      <td>mysql database performance innodb myisam</td>\n",
       "      <td>MyISAM versus InnoDB I'm working on a projects...</td>\n",
       "      <td>myisam versu innodb project involv lot databas...</td>\n",
       "      <td>myisam versu innodb</td>\n",
       "      <td>project involv lot databas write say insert re...</td>\n",
       "      <td>mysql databas perform innodb myisam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recommended SQL database design for tags or ta...</td>\n",
       "      <td>I've heard of a few ways to implement tagging;...</td>\n",
       "      <td>sql database-design tags data-modeling tagging</td>\n",
       "      <td>Recommended SQL database design for tags or ta...</td>\n",
       "      <td>recommend sql databas design tag tag heard way...</td>\n",
       "      <td>recommend sql databas design tag tag</td>\n",
       "      <td>heard way implement tag map tabl tagid itemid ...</td>\n",
       "      <td>sql databas design tag data model tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the difference between #include &lt;filen...</td>\n",
       "      <td>What is the difference between using angle bra...</td>\n",
       "      <td>c++ c include header-files c-preprocessor</td>\n",
       "      <td>What is the difference between #include &lt;filen...</td>\n",
       "      <td>differ includ filenam includ filenam differ an...</td>\n",
       "      <td>differ includ filenam includ filenam</td>\n",
       "      <td>differ angl bracket quot includ direct includ ...</td>\n",
       "      <td>includ header preprocessor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I size a UITextView to its content?</td>\n",
       "      <td>Is there a good way to adjust the size of a UI...</td>\n",
       "      <td>ios cocoa-touch autolayout uikit uitextview</td>\n",
       "      <td>How do I size a UITextView to its content? Is ...</td>\n",
       "      <td>size uitextview content good way adjust size u...</td>\n",
       "      <td>size uitextview content</td>\n",
       "      <td>good way adjust size uitextview conform conten...</td>\n",
       "      <td>io cocoa touch autolayout uikit uitextview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Batch file to delete files older than N days</td>\n",
       "      <td>I am looking for a way to delete all files old...</td>\n",
       "      <td>windows date batch-file file-io cmd</td>\n",
       "      <td>Batch file to delete files older than N days I...</td>\n",
       "      <td>batch delet older day look way delet older day...</td>\n",
       "      <td>batch delet older day</td>\n",
       "      <td>look way delet older day batch search around w...</td>\n",
       "      <td>window date batch cmd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                               MyISAM versus InnoDB   \n",
       "1  Recommended SQL database design for tags or ta...   \n",
       "2  What is the difference between #include <filen...   \n",
       "3         How do I size a UITextView to its content?   \n",
       "4       Batch file to delete files older than N days   \n",
       "\n",
       "                                                Body  \\\n",
       "0  I'm working on a projects which involves a lot...   \n",
       "1  I've heard of a few ways to implement tagging;...   \n",
       "2  What is the difference between using angle bra...   \n",
       "3  Is there a good way to adjust the size of a UI...   \n",
       "4  I am looking for a way to delete all files old...   \n",
       "\n",
       "                                             Tags  \\\n",
       "0        mysql database performance innodb myisam   \n",
       "1  sql database-design tags data-modeling tagging   \n",
       "2       c++ c include header-files c-preprocessor   \n",
       "3     ios cocoa-touch autolayout uikit uitextview   \n",
       "4             windows date batch-file file-io cmd   \n",
       "\n",
       "                                          title_body  \\\n",
       "0  MyISAM versus InnoDB I'm working on a projects...   \n",
       "1  Recommended SQL database design for tags or ta...   \n",
       "2  What is the difference between #include <filen...   \n",
       "3  How do I size a UITextView to its content? Is ...   \n",
       "4  Batch file to delete files older than N days I...   \n",
       "\n",
       "                                    clean_title_body  \\\n",
       "0  myisam versu innodb project involv lot databas...   \n",
       "1  recommend sql databas design tag tag heard way...   \n",
       "2  differ includ filenam includ filenam differ an...   \n",
       "3  size uitextview content good way adjust size u...   \n",
       "4  batch delet older day look way delet older day...   \n",
       "\n",
       "                            clean_title  \\\n",
       "0                   myisam versu innodb   \n",
       "1  recommend sql databas design tag tag   \n",
       "2  differ includ filenam includ filenam   \n",
       "3               size uitextview content   \n",
       "4                 batch delet older day   \n",
       "\n",
       "                                          clean_body  \\\n",
       "0  project involv lot databas write say insert re...   \n",
       "1  heard way implement tag map tabl tagid itemid ...   \n",
       "2  differ angl bracket quot includ direct includ ...   \n",
       "3  good way adjust size uitextview conform conten...   \n",
       "4  look way delet older day batch search around w...   \n",
       "\n",
       "                                   clean_tags  \n",
       "0         mysql databas perform innodb myisam  \n",
       "1       sql databas design tag data model tag  \n",
       "2                  includ header preprocessor  \n",
       "3  io cocoa touch autolayout uikit uitextview  \n",
       "4                       window date batch cmd  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chemin de notre fichier source\n",
    "data = \"cleaned/\"\n",
    "os.listdir(data)\n",
    "\n",
    "#Chargement du Dataframe\n",
    "fn = data + 'final_df.csv'\n",
    "df = pd.read_csv(fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>2. approche Bag of Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On drop les NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>title_body</th>\n",
       "      <th>clean_title_body</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>clean_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>What does if __name__ == \"__main__\": do?</td>\n",
       "      <td>What does this do, and why should one include ...</td>\n",
       "      <td>python namespaces main python-module idioms</td>\n",
       "      <td>What does if __name__ == \"__main__\": do? What ...</td>\n",
       "      <td>one includ statement print hello world</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one includ statement print hello world</td>\n",
       "      <td>python namespac main python modul idiom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Why do we use Base64?</td>\n",
       "      <td>Wikipedia says\\n\\nBase64 encoding schemes are ...</td>\n",
       "      <td>algorithm character-encoding binary ascii base64</td>\n",
       "      <td>Why do we use Base64? Wikipedia says\\n\\nBase64...</td>\n",
       "      <td>wikipedia say encod scheme commonli need encod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia say encod scheme commonli need encod...</td>\n",
       "      <td>algorithm charact encod binari ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>What are .a and .so files?</td>\n",
       "      <td>I'm currently trying to port a C application t...</td>\n",
       "      <td>c unix compilation shared-libraries .a</td>\n",
       "      <td>What are .a and .so files? I'm currently tryin...</td>\n",
       "      <td>current tri port applic aix confus build run a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>current tri port applic aix confus build run a...</td>\n",
       "      <td>unix compil share librari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>push_back vs emplace_back</td>\n",
       "      <td>I'm a bit confused regarding the difference be...</td>\n",
       "      <td>c++ visual-studio-2010 stl c++11 move-semantics</td>\n",
       "      <td>push_back vs emplace_back I'm a bit confused r...</td>\n",
       "      <td>bit confus regard differ void type void const ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bit confus regard differ void type void const ...</td>\n",
       "      <td>visual studio stl move semant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Title  \\\n",
       "54    What does if __name__ == \"__main__\": do?   \n",
       "224                      Why do we use Base64?   \n",
       "909                 What are .a and .so files?   \n",
       "1241                 push_back vs emplace_back   \n",
       "\n",
       "                                                   Body  \\\n",
       "54    What does this do, and why should one include ...   \n",
       "224   Wikipedia says\\n\\nBase64 encoding schemes are ...   \n",
       "909   I'm currently trying to port a C application t...   \n",
       "1241  I'm a bit confused regarding the difference be...   \n",
       "\n",
       "                                                  Tags  \\\n",
       "54         python namespaces main python-module idioms   \n",
       "224   algorithm character-encoding binary ascii base64   \n",
       "909             c unix compilation shared-libraries .a   \n",
       "1241   c++ visual-studio-2010 stl c++11 move-semantics   \n",
       "\n",
       "                                             title_body  \\\n",
       "54    What does if __name__ == \"__main__\": do? What ...   \n",
       "224   Why do we use Base64? Wikipedia says\\n\\nBase64...   \n",
       "909   What are .a and .so files? I'm currently tryin...   \n",
       "1241  push_back vs emplace_back I'm a bit confused r...   \n",
       "\n",
       "                                       clean_title_body clean_title  \\\n",
       "54               one includ statement print hello world         NaN   \n",
       "224   wikipedia say encod scheme commonli need encod...         NaN   \n",
       "909   current tri port applic aix confus build run a...         NaN   \n",
       "1241  bit confus regard differ void type void const ...         NaN   \n",
       "\n",
       "                                             clean_body  \\\n",
       "54               one includ statement print hello world   \n",
       "224   wikipedia say encod scheme commonli need encod...   \n",
       "909   current tri port applic aix confus build run a...   \n",
       "1241  bit confus regard differ void type void const ...   \n",
       "\n",
       "                                   clean_tags  \n",
       "54    python namespac main python modul idiom  \n",
       "224      algorithm charact encod binari ascii  \n",
       "909                 unix compil share librari  \n",
       "1241            visual studio stl move semant  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_NaN = df.isnull()\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "rows_with_NaN = df[row_has_NaN]\n",
    "rows_with_NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVEC TITLE POUR LE MOMENT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "CountVec = CountVectorizer(ngram_range=(1,1)) # to use bigrams ngram_range=(2,2)\n",
    "\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform(df[\"clean_title\"])\n",
    "\n",
    "#create dataframe\n",
    "X_bag_of_words=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abl</th>\n",
       "      <th>abort</th>\n",
       "      <th>absolut</th>\n",
       "      <th>abstract</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accessor</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>yea</th>\n",
       "      <th>year</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yum</th>\n",
       "      <th>yyyi</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeromq</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abbrevi  abl  abort  absolut  abstract  accent  accept  access  \\\n",
       "1115        0    0      0        0         1       0       0       0   \n",
       "969         0    0      0        0         0       0       0       0   \n",
       "501         0    0      0        0         0       0       0       0   \n",
       "1370        0    0      0        0         0       0       0       0   \n",
       "857         0    0      0        0         0       0       0       0   \n",
       "254         0    0      0        0         0       0       0       0   \n",
       "340         0    0      0        0         0       0       0       0   \n",
       "1191        0    0      0        0         0       0       0       0   \n",
       "703         0    0      0        0         0       0       0       0   \n",
       "109         0    0      0        0         0       0       0       0   \n",
       "\n",
       "      accessor  accord  ...  yea  year  youtub  yum  yyyi  zero  zeromq  \\\n",
       "1115         0       0  ...    0     0       0    0     0     0       0   \n",
       "969          0       0  ...    0     0       0    0     0     0       0   \n",
       "501          0       0  ...    0     0       0    0     0     0       0   \n",
       "1370         0       0  ...    0     0       0    0     0     0       0   \n",
       "857          0       0  ...    0     0       0    0     0     0       0   \n",
       "254          0       0  ...    0     0       0    0     0     0       0   \n",
       "340          0       0  ...    0     0       0    0     0     0       0   \n",
       "1191         0       0  ...    0     0       0    0     0     0       0   \n",
       "703          0       0  ...    0     0       0    0     0     0       0   \n",
       "109          0       0  ...    0     0       0    0     0     0       0   \n",
       "\n",
       "      zombi  zone  zoom  \n",
       "1115      0     0     0  \n",
       "969       0     0     0  \n",
       "501       0     0     0  \n",
       "1370      0     0     0  \n",
       "857       0     0     0  \n",
       "254       0     0     0  \n",
       "340       0     0     0  \n",
       "1191      0     0     0  \n",
       "703       0     0     0  \n",
       "109       0     0     0  \n",
       "\n",
       "[10 rows x 2030 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affichage\n",
    "X_bag_of_words.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>3. approche n-gram </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop_words: Since CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words like ‘the’, ‘and’, etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don’t take those words into account. Stop words are just a list of words you don’t want to use as features. You can set the parameter stop_words=’english’ to use a built-in list. Alternatively you can set stop_words equal to some custom list. This parameter defaults to None.\n",
    "ngram_range: An n-gram is just a string of n words in a row. E.g. the sentence ‘I am Groot’ contains the 2-grams ‘I am’ and ‘am Groot’. The sentence is itself a 3-gram. Set the parameter ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram_range is (1,1).\n",
    "In a recent project where I modeled job postings online, I found that including 2-grams as features boosted my model’s predictive power significantly. This makes intuitive sense; many job titles such as ‘data scientist’, ‘data engineer’, and ‘data analyst’ are 2 words long.\n",
    "min_df, max_df: These are the minimum and maximum document frequencies words/n-grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit. min_df defaults to 1 (int) and max_df defaults to 1.0 (float).\n",
    "max_features: This parameter is pretty self-explanatory. The CountVectorizer will choose the words/features that occur most frequently to be in its’ vocabulary and drop everything else.\n",
    "You would set these parameters when initializing your CountVectorizer object as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "CountVec = CountVectorizer(ngram_range=(2,2)) # to use bigrams ngram_range=(2,2)\n",
    "\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform(df[\"clean_title\"])\n",
    "\n",
    "#create dataframe\n",
    "X_bigram=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbrevi net</th>\n",
       "      <th>abl scroll</th>\n",
       "      <th>abort could</th>\n",
       "      <th>absolut path</th>\n",
       "      <th>absolut posit</th>\n",
       "      <th>abstract class</th>\n",
       "      <th>abstract method</th>\n",
       "      <th>accent normal</th>\n",
       "      <th>accept attribut</th>\n",
       "      <th>accept best</th>\n",
       "      <th>...</th>\n",
       "      <th>youtub api</th>\n",
       "      <th>youtub video</th>\n",
       "      <th>yum instal</th>\n",
       "      <th>yyyi javascript</th>\n",
       "      <th>zero date</th>\n",
       "      <th>zombi exist</th>\n",
       "      <th>zone best</th>\n",
       "      <th>zone issu</th>\n",
       "      <th>zoom input</th>\n",
       "      <th>zoom mobil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5937 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abbrevi net  abl scroll  abort could  absolut path  absolut posit  \\\n",
       "1393            0           0            0             0              0   \n",
       "1304            0           0            0             0              0   \n",
       "1308            0           0            0             0              0   \n",
       "1155            0           0            0             0              0   \n",
       "1518            0           0            0             0              0   \n",
       "199             0           0            0             0              0   \n",
       "112             0           0            0             0              0   \n",
       "758             0           0            0             0              0   \n",
       "1316            0           0            0             0              0   \n",
       "1122            0           0            0             0              0   \n",
       "\n",
       "      abstract class  abstract method  accent normal  accept attribut  \\\n",
       "1393               0                0              0                0   \n",
       "1304               0                0              0                0   \n",
       "1308               0                0              0                0   \n",
       "1155               0                0              0                0   \n",
       "1518               0                0              0                0   \n",
       "199                0                0              0                0   \n",
       "112                0                0              0                0   \n",
       "758                0                0              0                0   \n",
       "1316               0                0              0                0   \n",
       "1122               0                0              0                0   \n",
       "\n",
       "      accept best  ...  youtub api  youtub video  yum instal  yyyi javascript  \\\n",
       "1393            0  ...           0             0           0                0   \n",
       "1304            0  ...           0             0           0                0   \n",
       "1308            0  ...           0             0           1                0   \n",
       "1155            0  ...           0             0           0                0   \n",
       "1518            0  ...           0             0           0                0   \n",
       "199             0  ...           0             0           0                0   \n",
       "112             0  ...           0             0           0                0   \n",
       "758             0  ...           0             0           0                0   \n",
       "1316            0  ...           0             0           0                0   \n",
       "1122            0  ...           0             0           0                0   \n",
       "\n",
       "      zero date  zombi exist  zone best  zone issu  zoom input  zoom mobil  \n",
       "1393          0            0          0          0           0           0  \n",
       "1304          0            0          0          0           0           0  \n",
       "1308          0            0          0          0           0           0  \n",
       "1155          0            0          0          0           0           0  \n",
       "1518          0            0          0          0           0           0  \n",
       "199           0            0          0          0           0           0  \n",
       "112           0            0          0          0           0           0  \n",
       "758           0            0          0          0           0           0  \n",
       "1316          0            0          0          0           0           0  \n",
       "1122          0            0          0          0           0           0  \n",
       "\n",
       "[10 rows x 5937 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bigram.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRB == Matrice creuse !, peut être regler (sur le net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>4. approche TF - IDF </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    " \n",
    "#transform\n",
    "result = tfidf.fit_transform(df[\"clean_title\"])\n",
    "\n",
    "#create dataframe\n",
    "X_tfidf=pd.DataFrame(result.toarray(),columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abl</th>\n",
       "      <th>abort</th>\n",
       "      <th>absolut</th>\n",
       "      <th>abstract</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accessor</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>yea</th>\n",
       "      <th>year</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yum</th>\n",
       "      <th>yyyi</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeromq</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abbrevi  abl  abort   absolut  abstract  accent  accept    access  \\\n",
       "1611      0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "827       0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "219       0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "1296      0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.231873   \n",
       "928       0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "1125      0.0  0.0    0.0  0.376043       0.0     0.0     0.0  0.000000   \n",
       "1360      0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "556       0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "1637      0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "666       0.0  0.0    0.0  0.000000       0.0     0.0     0.0  0.000000   \n",
       "\n",
       "      accessor  accord  ...  yea  year  youtub  yum  yyyi  zero  zeromq  \\\n",
       "1611       0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "827        0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "219        0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "1296       0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "928        0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "1125       0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "1360       0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "556        0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "1637       0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "666        0.0     0.0  ...  0.0   0.0     0.0  0.0   0.0   0.0     0.0   \n",
       "\n",
       "      zombi  zone  zoom  \n",
       "1611    0.0   0.0   0.0  \n",
       "827     0.0   0.0   0.0  \n",
       "219     0.0   0.0   0.0  \n",
       "1296    0.0   0.0   0.0  \n",
       "928     0.0   0.0   0.0  \n",
       "1125    0.0   0.0   0.0  \n",
       "1360    0.0   0.0   0.0  \n",
       "556     0.0   0.0   0.0  \n",
       "1637    0.0   0.0   0.0  \n",
       "666     0.0   0.0   0.0  \n",
       "\n",
       "[10 rows x 2030 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF (autre méthode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>5. Plongement de mots avec Word2Vec et  Doc2Vec</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>1. Documentation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html#Word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Applying the word embedding to a text classification task</b>\n",
    "Now, we have our word representation, a vector for every word in our vocabulary. But we need to deal with full sentences so we need to create a sentence embedding, basically we need a vector that represent the whole sentence and every feature in the vector will be based on the word embeddings. There are many posibilities and we are notr going to cover this topic, so we apply a very simple method: the ith value in the sentence embedding will be the mean of the ith values in the word embedding of all the words in the sentence.\n",
    "\n",
    "We will create a class that will contain our vocabulary and glove vectors and then it will transform every review (a sentence in our example) to a vector representation as we describe previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sympa : https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>2. Word2Vec</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = grey>a. Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tokens\n",
    "tokens = df[\"clean_title\"].values.tolist()\n",
    "\n",
    "# Tokenizing strings in list of strings\n",
    "res = [sub.split() for sub in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 14:56:07,156 : INFO : collecting all words and their counts\n",
      "2022-10-17 14:56:07,156 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-10-17 14:56:07,158 : INFO : collected 2030 word types from a corpus of 8328 raw words and 1671 sentences\n",
      "2022-10-17 14:56:07,159 : INFO : Creating a fresh vocabulary\n",
      "2022-10-17 14:56:07,162 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 1028 unique words (50.64039408866995%% of original 2030, drops 1002)', 'datetime': '2022-10-17T14:56:07.162323', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:56:07,163 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 7326 word corpus (87.96829971181556%% of original 8328, drops 1002)', 'datetime': '2022-10-17T14:56:07.163321', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:56:07,170 : INFO : deleting the raw counts dictionary of 2030 items\n",
      "2022-10-17 14:56:07,171 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2022-10-17 14:56:07,172 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 6535.487264669541 word corpus (89.2%% of prior 7326)', 'datetime': '2022-10-17T14:56:07.172335', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:56:07,182 : INFO : estimated required memory for 1028 words and 100 dimensions: 1336400 bytes\n",
      "2022-10-17 14:56:07,183 : INFO : resetting layer weights\n",
      "2022-10-17 14:56:07,184 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-10-17T14:56:07.184324', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2022-10-17 14:56:07,185 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 1028 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-10-17T14:56:07.185322', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-10-17 14:56:07,193 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,193 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,194 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,194 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,195 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,195 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,196 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,199 : INFO : EPOCH - 1 : training on 8328 raw words (6495 effective words) took 0.0s, 694237 effective words/s\n",
      "2022-10-17 14:56:07,207 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,208 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,209 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,209 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,210 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,210 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,211 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,211 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,213 : INFO : EPOCH - 2 : training on 8328 raw words (6539 effective words) took 0.0s, 843829 effective words/s\n",
      "2022-10-17 14:56:07,222 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,223 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,224 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,224 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,225 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,225 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,226 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,231 : INFO : EPOCH - 3 : training on 8328 raw words (6526 effective words) took 0.0s, 553342 effective words/s\n",
      "2022-10-17 14:56:07,240 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,241 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,241 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,242 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,242 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,243 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,243 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,245 : INFO : EPOCH - 4 : training on 8328 raw words (6530 effective words) took 0.0s, 857439 effective words/s\n",
      "2022-10-17 14:56:07,252 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,253 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,253 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,254 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,254 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,255 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,259 : INFO : EPOCH - 5 : training on 8328 raw words (6541 effective words) took 0.0s, 700892 effective words/s\n",
      "2022-10-17 14:56:07,260 : INFO : Word2Vec lifecycle event {'msg': 'training on 41640 raw words (32631 effective words) took 0.1s, 433940 effective words/s', 'datetime': '2022-10-17T14:56:07.260493', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-10-17 14:56:07,261 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=1028, vector_size=100, alpha=0.025)', 'datetime': '2022-10-17T14:56:07.261496', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-10-17 14:56:07,262 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2022-10-17 14:56:07,262 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 1028 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-10-17T14:56:07.262494', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-10-17 14:56:07,316 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,317 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,317 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,318 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,318 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,318 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,319 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,319 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,324 : INFO : EPOCH - 1 : training on 8328 raw words (6516 effective words) took 0.0s, 671400 effective words/s\n",
      "2022-10-17 14:56:07,332 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,333 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,333 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,334 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,334 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,335 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,335 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,339 : INFO : EPOCH - 2 : training on 8328 raw words (6506 effective words) took 0.0s, 831045 effective words/s\n",
      "2022-10-17 14:56:07,346 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,347 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,347 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,348 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,349 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,349 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,350 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,353 : INFO : EPOCH - 3 : training on 8328 raw words (6527 effective words) took 0.0s, 783130 effective words/s\n",
      "2022-10-17 14:56:07,362 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,363 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,364 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,364 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,366 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,366 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,367 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,369 : INFO : EPOCH - 4 : training on 8328 raw words (6561 effective words) took 0.0s, 679601 effective words/s\n",
      "2022-10-17 14:56:07,374 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,375 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,376 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,376 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,376 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,377 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,380 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,381 : INFO : EPOCH - 5 : training on 8328 raw words (6531 effective words) took 0.0s, 792587 effective words/s\n",
      "2022-10-17 14:56:07,387 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,388 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,388 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,389 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,389 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,389 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,389 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,393 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,394 : INFO : EPOCH - 6 : training on 8328 raw words (6557 effective words) took 0.0s, 785957 effective words/s\n",
      "2022-10-17 14:56:07,401 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,402 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,402 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,402 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,403 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,403 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,404 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,404 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,408 : INFO : EPOCH - 7 : training on 8328 raw words (6524 effective words) took 0.0s, 772693 effective words/s\n",
      "2022-10-17 14:56:07,413 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,414 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,415 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,415 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,415 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,415 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,416 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,417 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,420 : INFO : EPOCH - 8 : training on 8328 raw words (6557 effective words) took 0.0s, 805330 effective words/s\n",
      "2022-10-17 14:56:07,426 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,426 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,427 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,427 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,427 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,428 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,428 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,429 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,430 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,431 : INFO : EPOCH - 9 : training on 8328 raw words (6544 effective words) took 0.0s, 929862 effective words/s\n",
      "2022-10-17 14:56:07,437 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-10-17 14:56:07,439 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-10-17 14:56:07,439 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-10-17 14:56:07,439 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-10-17 14:56:07,440 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-10-17 14:56:07,440 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-10-17 14:56:07,440 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-17 14:56:07,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-17 14:56:07,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-17 14:56:07,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-17 14:56:07,445 : INFO : EPOCH - 10 : training on 8328 raw words (6512 effective words) took 0.0s, 649511 effective words/s\n",
      "2022-10-17 14:56:07,446 : INFO : Word2Vec lifecycle event {'msg': 'training on 83280 raw words (65335 effective words) took 0.2s, 356291 effective words/s', 'datetime': '2022-10-17T14:56:07.446494', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65335, 83280)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = gensim.models.Word2Vec(res, vector_size = 100, window=10, min_count=2, workers=10)\n",
    "\n",
    "# Train the model\n",
    "model.train(res,total_examples=len(res),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = grey>b. Similarité entre les mots</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1653399 ,  0.35387492,  0.06272807,  0.07683088,  0.10408764,\n",
       "       -0.6008398 ,  0.15346728,  0.6884075 , -0.17324859, -0.06603417,\n",
       "       -0.21245202, -0.42875198, -0.05762694,  0.12825197,  0.11886377,\n",
       "       -0.22092761,  0.00969438, -0.3866519 , -0.03466053, -0.58453363,\n",
       "        0.16916226,  0.12993272,  0.18430786, -0.12366612, -0.1419507 ,\n",
       "        0.10667771, -0.32484126, -0.34951004, -0.2210636 ,  0.06093941,\n",
       "        0.33851004,  0.16391717,  0.06453181, -0.06206826, -0.19850117,\n",
       "        0.3671479 ,  0.00203453, -0.3418623 , -0.2773217 , -0.6720459 ,\n",
       "        0.04736494, -0.25128263, -0.03233613, -0.07927433,  0.2724138 ,\n",
       "       -0.07419963, -0.19989623, -0.09086411,  0.17745633,  0.2004094 ,\n",
       "        0.18448474, -0.3341665 ,  0.00757688,  0.02441483, -0.2961934 ,\n",
       "        0.24906698,  0.12626415, -0.01691405, -0.38377443,  0.06079949,\n",
       "        0.14948966,  0.14545004, -0.12239191, -0.12899993, -0.4265291 ,\n",
       "        0.15256357,  0.14578624,  0.23557256, -0.35711434,  0.37386164,\n",
       "       -0.27130157,  0.0718662 ,  0.32271355, -0.09683996,  0.31972897,\n",
       "        0.15297279, -0.01964416, -0.13763043, -0.3324543 ,  0.12076118,\n",
       "       -0.13149372, -0.06665724, -0.3375857 ,  0.5007649 , -0.00913792,\n",
       "        0.12886024,  0.03032141,  0.38810018,  0.38217175,  0.04720031,\n",
       "        0.40777043,  0.17573068, -0.04334519, -0.00543819,  0.45611742,\n",
       "        0.3689422 ,  0.15492427, -0.44587487,  0.17395276, -0.11394033],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observer le vecteur de \"use\"\n",
    "model.wv[\"python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load', 0.9995328783988953),\n",
       " ('io', 0.9995101690292358),\n",
       " ('valu', 0.9995100498199463),\n",
       " ('app', 0.9994659423828125),\n",
       " ('json', 0.9994465112686157),\n",
       " ('error', 0.9994300007820129)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Les 6 mots les plus similaire à python\n",
    "w1 = [\"python\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('valu', 0.9995467066764832),\n",
       " ('load', 0.9995236992835999),\n",
       " ('text', 0.9995140433311462),\n",
       " ('error', 0.9995064735412598),\n",
       " ('chang', 0.9994917511940002),\n",
       " ('differ', 0.9994842410087585)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Les 6 mots les plus similaire à python\n",
    "w1 = [\"java\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993716"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarité entre 2 mots :\n",
    "model.wv.similarity(w1=\"java\", w2=\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994277"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarité entre 2 mots :\n",
    "model.wv.similarity(w1=\"java\", w2=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarité entre 2 mots identique :\n",
    "model.wv.similarity(w1=\"java\", w2=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': ['load', 'io', 'valu', 'app', 'json']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['python']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = grey>c. Vizualisation TSNE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>3. Doc2Vec</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBOW is the doc2vec model analogous to Skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "We will vary the following parameters:\n",
    "\n",
    "If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
    "300- dimensional feature vectors.\n",
    "min_count=2, ignores all words with total frequency lower than this.\n",
    "negative=5 , specifies how many “noise words” should be drawn.\n",
    "hs=0 , and negative is non-zero, negative sampling will be used.\n",
    "sample=0 , the threshold for configuring which higher-frequency words are randomly down sampled.\n",
    "workers=cores , use these many worker threads to train the model (=faster training with multicore machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vérification du nb de coeur dispo\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "data = df[\"clean_title\"]\n",
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words='format unsign long long int printf', tags=['200'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 14:59:02,293 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d20,n5,w5,s0.001,t3)', 'datetime': '2022-10-17T14:59:02.293032', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-10-17 14:59:02,294 : INFO : collecting all words and their counts\n",
      "2022-10-17 14:59:02,295 : WARNING : Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n",
      "2022-10-17 14:59:02,296 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2022-10-17 14:59:02,307 : INFO : collected 27 word types and 1671 unique tags from a corpus of 1671 examples and 53003 words\n",
      "2022-10-17 14:59:02,308 : INFO : Creating a fresh vocabulary\n",
      "2022-10-17 14:59:02,309 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 27 unique words (100.0%% of original 27, drops 0)', 'datetime': '2022-10-17T14:59:02.309019', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:59:02,310 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 53003 word corpus (100.0%% of original 53003, drops 0)', 'datetime': '2022-10-17T14:59:02.310019', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:59:02,311 : INFO : deleting the raw counts dictionary of 27 items\n",
      "2022-10-17 14:59:02,311 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2022-10-17 14:59:02,311 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 9274.455363514644 word corpus (17.5%% of prior 53003)', 'datetime': '2022-10-17T14:59:02.311020', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-10-17 14:59:02,312 : INFO : estimated required memory for 27 words and 20 dimensions: 485700 bytes\n",
      "2022-10-17 14:59:02,313 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1671,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_title\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la matrice des vecteurs de chaque phrases\n",
    "X_doc2vec = np.zeros((1671,20)) #20 correpsond au vector_size\n",
    "\n",
    "for i in range(1671):\n",
    "    X_doc2vec[i] = model[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02615408, -0.02989564, -0.04940378, ...,  0.04612811,\n",
       "        -0.04750933, -0.01729044],\n",
       "       [-0.01884955,  0.01303691, -0.02845779, ..., -0.04271271,\n",
       "        -0.01801307,  0.00865812],\n",
       "       [-0.0102846 , -0.03615022,  0.02092299, ...,  0.01767989,\n",
       "        -0.02875156,  0.04407407],\n",
       "       ...,\n",
       "       [-0.0092081 ,  0.0453164 , -0.03758141, ...,  0.01410308,\n",
       "        -0.04667521, -0.01426193],\n",
       "       [ 0.04282513,  0.04463822,  0.00622153, ..., -0.02043825,\n",
       "        -0.00414624,  0.03259372],\n",
       "       [ 0.03273497,  0.02404989, -0.00732609, ..., -0.04914112,\n",
       "         0.02745314, -0.01130791]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>4. BERT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>3. USE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Green>Partie 2 : Approche non supervisée : LDA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>1. Préléminaires</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le monde actuel, où la quantité de texte non structuré, augmente drastiquement (commentaires, articles de blog, etc.), il serait vraiment utile d’avoir des outils qui permettent de structurer automatiquement l’information, de manière à pouvoir rapidement accéder à ce qui nous intéresse, filtrer le bruit mais aussi détecter l’apparition de nouveau sujet d’intérêts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première méthode vraiment efficace est nommé LDA (Latent Dirichlet Allocation). C’est une méthode non-supervisée générative qui se base sur les hypothèses suivantes :\n",
    "\n",
    "Chaque document du corpus est un ensemble de mots sans ordre (bag-of-words) ;\n",
    "\n",
    "Chaque document m aborde un certain nombre de thèmes dans différentes proportions qui lui sont propres p(θm) ;\n",
    "\n",
    "Chaque mot possède une distribution associée à chaque thème p(ϕk) . On peut ainsi représenter chaque thème par une probabilité sur chaque mot.\n",
    "\n",
    " zn représente le thème du mot wn\n",
    "\n",
    "Puisque l'on a accès uniquement aux documents, on doit déterminer quels sont les thèmes, les distributions de chaque mot sur les thèmes, la fréquence d’apparition de chaque thème sur le corpus.\n",
    "\n",
    "Une représentation formelle sous forme de modèle probabiliste graphique est la suivante :\n",
    "\n",
    "Modèle probabiliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/tags-recommendation-algorithm-using-latent-dirichlet-allocation-lda-3f844abf99d7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>2. Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechercher le nombre de topics idéal : perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of topics\n",
    "no_topics = 20\n",
    "\n",
    "#Initialize the model\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>3. Topics vizualisation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Topic 0:\n",
      "html loop attribut final link\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 1:\n",
      "remov input import modul part\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 2:\n",
      "swift angular class react member\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 3:\n",
      "mean list convert php warn\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 4:\n",
      "type databas enum rail framework\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 5:\n",
      "class static librari creat maven\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 6:\n",
      "http request linq join httpclient\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 7:\n",
      "form script return shell enter\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 8:\n",
      "date java format time stream\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 9:\n",
      "keyword document docker heap templat\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 10:\n",
      "sql server updat variabl declar\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 11:\n",
      "algorithm pipelin utf complet asset\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 12:\n",
      "view method mock long redux\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 13:\n",
      "browser perform json page deseri\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 14:\n",
      "std jpa django hibern make\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 15:\n",
      "origin allow access control grep\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 16:\n",
      "certif connect includ disabl spring\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 17:\n",
      "array git javascript work object\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 18:\n",
      "panda datafram column select mysql\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Topic 19:\n",
      "file string get python java\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# use the function\n",
    "display_topics(lda, tfidf.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualisation 2D des topics LDA - librairie à trouver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>4. recommandation de Tags</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Green>Partie 3 : Approche supervisée</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>1. préparation ML</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize y\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = df[\"clean_tags\"]\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_bin = multilabel_binarizer.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anthony\\Documents\\40 - Formation Opencllassroom\\20_Projets\\05 - categorisation question\\05---categorisation-question\\second one.ipynb Cell 81\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anthony/Documents/40%20-%20Formation%20Opencllassroom/20_Projets/05%20-%20categorisation%20question/05---categorisation-question/second%20one.ipynb#Y203sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 1, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = purple>2. One vs Rest</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_jacard(y_true,y_pred):\n",
    "    '''\n",
    "    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n",
    "    '''\n",
    "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
    "    \n",
    "    return jacard.mean()*100\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "svc = LinearSVC()\n",
    "perceptron = Perceptron()\n",
    "pac = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_jacard(y_true,y_pred):\n",
    "    '''\n",
    "    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n",
    "    '''\n",
    "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
    "    \n",
    "    return jacard.mean()*100\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
    "    print(\"---\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  DummyClassifier\n",
      "Jacard score: 66.04826123867727\n",
      "Hamming loss: 24.079601990049753\n",
      "---\n",
      "Clf:  SGDClassifier\n",
      "Jacard score: 63.543849425818635\n",
      "Hamming loss: 24.38916528468767\n",
      "---\n",
      "Clf:  LogisticRegression\n",
      "Jacard score: 68.35959147224047\n",
      "Hamming loss: 20.829187396351575\n",
      "---\n",
      "Clf:  MultinomialNB\n",
      "Jacard score: 68.00116207098525\n",
      "Hamming loss: 22.631288004422334\n",
      "---\n",
      "Clf:  LinearSVC\n",
      "Jacard score: 59.99437385867632\n",
      "Hamming loss: 25.62741846323936\n",
      "---\n",
      "Clf:  Perceptron\n",
      "Jacard score: 64.73060644639904\n",
      "Hamming loss: 24.267551133222774\n",
      "---\n",
      "Clf:  PassiveAggressiveClassifier\n",
      "Jacard score: 63.55365606249597\n",
      "Hamming loss: 24.367053620784965\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bag_of_words, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split\n",
    "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  DummyClassifier\n",
      "Jacard score: 66.04826123867727\n",
      "Hamming loss: 24.079601990049753\n",
      "---\n",
      "Clf:  SGDClassifier\n",
      "Jacard score: 63.11300089813098\n",
      "Hamming loss: 24.190160309563293\n",
      "---\n",
      "Clf:  LogisticRegression\n",
      "Jacard score: 63.767281816089636\n",
      "Hamming loss: 23.88059701492537\n",
      "---\n",
      "Clf:  MultinomialNB\n",
      "Jacard score: 54.49373399140548\n",
      "Hamming loss: 34.80375898286346\n",
      "---\n",
      "Clf:  LinearSVC\n",
      "Jacard score: 57.12838042487781\n",
      "Hamming loss: 26.93200663349917\n",
      "---\n",
      "Clf:  Perceptron\n",
      "Jacard score: 64.71770090523844\n",
      "Hamming loss: 23.4383637368712\n",
      "---\n",
      "Clf:  PassiveAggressiveClassifier\n",
      "Jacard score: 61.592757134275544\n",
      "Hamming loss: 24.621337755666115\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bigram, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split\n",
    "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  DummyClassifier\n",
      "Jacard score: 62.97684031113844\n",
      "Hamming loss: 25.18293369357199\n",
      "---\n",
      "Clf:  SGDClassifier\n",
      "Jacard score: 63.706455269862936\n",
      "Hamming loss: 24.33862433862434\n",
      "---\n",
      "Clf:  LogisticRegression\n",
      "Jacard score: 67.49166533376916\n",
      "Hamming loss: 21.524259822132162\n",
      "---\n",
      "Clf:  MultinomialNB\n",
      "Jacard score: 68.15872491504953\n",
      "Hamming loss: 21.13024878982326\n",
      "---\n",
      "Clf:  LinearSVC\n",
      "Jacard score: 66.5483812321326\n",
      "Hamming loss: 22.199707306090286\n",
      "---\n",
      "Clf:  Perceptron\n",
      "Jacard score: 61.66256800880734\n",
      "Hamming loss: 25.858381177530116\n",
      "---\n",
      "Clf:  PassiveAggressiveClassifier\n",
      "Jacard score: 61.38343639572752\n",
      "Hamming loss: 26.083530338849485\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split\n",
    "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  DummyClassifier\n",
      "Jacard score: 62.97684031113844\n",
      "Hamming loss: 25.18293369357199\n",
      "---\n",
      "Clf:  SGDClassifier\n",
      "Jacard score: 60.32981568391198\n",
      "Hamming loss: 25.959698300123833\n",
      "---\n",
      "Clf:  LogisticRegression\n",
      "Jacard score: 62.680551260379524\n",
      "Hamming loss: 25.126646403242148\n",
      "---\n",
      "Clf:  LinearSVC\n",
      "Jacard score: 62.02890429410657\n",
      "Hamming loss: 25.430597771023304\n",
      "---\n",
      "Clf:  Perceptron\n",
      "Jacard score: 50.16118981181707\n",
      "Hamming loss: 33.59225486885061\n",
      "---\n",
      "Clf:  PassiveAggressiveClassifier\n",
      "Jacard score: 62.13795241874279\n",
      "Hamming loss: 25.959698300123833\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split\n",
    "for classifier in [dummy, sgd, lr, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/koredla25/predicting-tags-for-the-questions-in-satckoverflow/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/roccoli/multi-label-classification-with-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taux de couverture tag réels et mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2'))\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0\n",
      "Macro f1 score : 0.5162143534462663\n",
      "Micro f1 scoore : 0.7944119312818577\n",
      "Hamming loss : 0.2407960199004975\n",
      "Accuracy : 0.005970149253731343\n",
      "Macro f1 score : 0.6404056428997704\n",
      "Micro f1 scoore : 0.7715725383161872\n",
      "Hamming loss : 0.24057490326147044\n",
      "Accuracy : 0.0\n",
      "Macro f1 score : 0.5697479552655418\n",
      "Micro f1 scoore : 0.8017179670722977\n",
      "Hamming loss : 0.21437258153676064\n",
      "Accuracy : 0.0029850746268656717\n",
      "Macro f1 score : 0.5643747255173416\n",
      "Micro f1 scoore : 0.7398273736128237\n",
      "Hamming loss : 0.25660585959093424\n",
      "Accuracy : 0.008955223880597015\n",
      "Macro f1 score : 0.6625667232977142\n",
      "Micro f1 scoore : 0.786222312538102\n",
      "Hamming loss : 0.2326147042564953\n",
      "Accuracy : 0.008955223880597015\n",
      "Macro f1 score : 0.6700164384499911\n",
      "Micro f1 scoore : 0.7928757254352611\n",
      "Hamming loss : 0.22885572139303484\n"
     ]
    }
   ],
   "source": [
    "for classifier in [dummy, sgd, lr, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
    "    print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
    "    print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
    "    print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0\n",
      "Macro f1 score : 0.5121572819048519\n",
      "Micro f1 scoore : 0.7237715803452857\n",
      "Hamming loss : 0.3219458264234384\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
    "print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
    "print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff762b79131b0a616db2e5d2e2f0cf802027d9b17e52f1566d363b2981a5f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
